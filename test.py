# import cv2
# from pyzbar.pyzbar import decode

# cap = cv2.VideoCapture(0)  # ê¸°ë³¸ ì¹´ë©”ë¼ ì‚¬ìš©

# print("QR/ë°”ì½”ë“œ ì¸ì‹ ëŒ€ê¸° ì¤‘... (ì°½ì—ì„œ 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ)")

# while True:
#     ret, frame = cap.read()
#     if not ret:
#         print("ì¹´ë©”ë¼ ì½ê¸° ì‹¤íŒ¨")
#         break

#     barcodes = decode(frame)
#     for barcode in barcodes:
#         data = barcode.data.decode('utf-8')
#         barcode_type = barcode.type  # QR Code, CODE128, EAN13 ë“±

#         print(f"[{barcode_type} ì¸ì‹ë¨] â–¶ {data}")

#         # ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
#         x, y, w, h = barcode.rect
#         cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

#         # ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë³´ì—¬ì£¼ê¸°
#         cv2.putText(frame, f"{barcode_type}: {data}", (x, y - 10),
#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50, 255, 50), 2)

#     cv2.imshow("QR/Barcode Scanner", frame)

#     if cv2.waitKey(1) & 0xFF == ord('q'):
#         break

# cap.release()
# cv2.destroyAllWindows()


import time
import re
start = time.time()
import requests
from bs4 import BeautifulSoup
import datetime
ISBN13 = ["9788931551167"]#, "9791169210911", "9791169213608", "9791169212151", "9791169212144", "9791169212137", "9791169211901"]




# ISBN13 = ""

# while True:
#     ret, frame = cap.read()
#     if not ret:
#         print("ì¹´ë©”ë¼ ì½ê¸° ì‹¤íŒ¨")
#         break

#     barcodes = decode(frame)
#     for barcode in barcodes:
#         data = barcode.data.decode('utf-8')
#         barcode_type = barcode.type  # QR Code, CODE128, EAN13 ë“±

#         print(f"[{barcode_type} ì¸ì‹ë¨] â–¶ {data}")
#         ISBN13 = data

#         # ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
#         x, y, w, h = barcode.rect
#         cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

#         # ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë³´ì—¬ì£¼ê¸°
#         cv2.putText(frame, f"{barcode_type}: {data}", (x, y - 10),
#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50, 255, 50), 2)

#     cv2.imshow("QR/Barcode Scanner", frame)

#     if cv2.waitKey(1) & 0xFF == ord('q') or ISBN13 != "":
#         break

# cap.release()
# cv2.destroyAllWindows()








'''
# start = time.time()
for isbn in ISBN13:
    print(f"/nğŸ“• ISBN ê²€ìƒ‰: {isbn}")

    try:
        # 1. ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼
        url = f"https://www.yes24.com/Product/Search?domain=BOOK&query={isbn}"
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.text, 'html.parser')

        # 2. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë„ì„œ ìƒì„¸ ë§í¬ ì¶”ì¶œ
        prd_info = soup.find('a', class_='gd_name')
        if not prd_info:
            print("âŒ ë„ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        url_detail = "https://www.yes24.com" + prd_info['href']
        print("ğŸ“˜ ìƒì„¸ ë§í¬:", url_detail)

        # 3. ìƒì„¸ í˜ì´ì§€ ìš”ì²­
        r = requests.get(url_detail, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.text, 'html.parser')

        # 4. ì œëª©
        title_tag = soup.find("h2", class_="gd_name")
        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

        # ì‘ê°€(ì €ì) ì´ë¦„ ì¶”ì¶œ
        author_tag = soup.select_one("span.gd_auth a")
        author = author_tag.get_text(strip=True) if author_tag else "ì €ì ì—†ìŒ"

        date_tag = soup.select_one("span.gd_date")
        pub_date = date_tag.get_text(strip=True) if date_tag else "ì¶œê°„ì¼ ì—†ìŒ"

        def parse_korean_date(date_str: str) -> datetime.date:
            match = re.search(r"(/d{4})ë…„/s*(/d{1,2})ì›”/s*(/d{1,2})ì¼", date_str)
            if match:
                year, month, day = map(int, match.groups())
                return datetime.date(year, month, day)
            else:
                return None
        
        print("ğŸ“˜ ìƒì„¸ ë§í¬:", url_detail)
        print("ğŸ“— ì œëª©:", title)
        print("ğŸ‘¤ ì €ì:", author)
        print("ì¶œíŒì¼ ",parse_korean_date(pub_date))

        time.sleep(1)  # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì‰¬ëŠ” ì‹œê°„

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", str(e))

'''

    # # ì´ë¯¸ì§€ íƒœê·¸ ì„ íƒ
    # img_tag = soup.find("img", class_="gImg")

    # # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    # if img_tag:
    #     img_url = img_tag.get("src")
    #     print("ì´ë¯¸ì§€ URL:", img_url)

    #     # ì´ë¯¸ì§€ ìš”ì²­ ë° ì €ì¥
    #     response = requests.get(img_url)
    #     if response.status_code == 200:
    #         with open("{0}.jpg".format(i), "wb") as f:
    #             f.write(response.content)
    #         print("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    #     else:
    #         print("ì´ë¯¸ì§€ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
    # else:
    #     print("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

end = time.time()
print(f"{end - start:.5f} sec")
# for i in ('infoset_introduce', 'infoset_toc'):
#     print(i + "/n")
#     prd_detail = soup.find('div', attrs={'id':{i}})
#     prd_tr_list = prd_detail.find_all('textarea')
#     print(prd_tr_list,"/n/n")

# prd_detail = soup.find('div', attrs={'id':'infoset_specific'})

# prd_tr_list = prd_detail.find_all('tr')

# for tr in prd_tr_list:
#     if tr.find('th').get_text() == "ìª½ìˆ˜, ë¬´ê²Œ, í¬ê¸°":
#         print(tr.find('td').get_text().split()[0])


isbn_seen = set()
isbn_unique = []
isbn_duplicates = set()

with open("C:/Users/nhlk1/OneDrive/ë°”íƒ• í™”ë©´/isbn_list.txt", "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        parts = line.split(",")
        if len(parts) != 2:
            continue  # ì˜ˆì™¸ ì²˜ë¦¬

        isbn = parts[1].strip()

        if isbn in isbn_seen:
            isbn_duplicates.add(isbn)
        else:
            isbn_seen.add(isbn)
            isbn_unique.append(isbn)

# âœ… ê²°ê³¼ ì¶œë ¥
print("ğŸ“š ì´ ISBN ê°œìˆ˜:", len(isbn_unique) + len(isbn_duplicates))
print("âœ… ê³ ìœ í•œ ISBN ìˆ˜:", len(isbn_unique))
print("â— ì¤‘ë³µ ISBN ìˆ˜:", len(isbn_duplicates))

if isbn_duplicates:
    print("/nğŸ“› ì¤‘ë³µëœ ISBN ëª©ë¡:")
    for d in isbn_duplicates:
        print("-", d)


